---
title: "HW02"
author: "Giuseppina Orefice, Alessandra Campanella"
date: "2024-12-3"
output:
  html_document: 
    toc: true
    toc_depth: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library("trackdown")
trackdown::upload_file("hmw02.Rmd")
```

# 1) Theory Conformal Prediction

In the ACI paper has provided a general description of both the conformal prediction and the adaptive conformal prediction. Conformal prediction gives a way to construct prediction interval that guarantees the so called "validity coverage". Let $Z_t = (X_r, Y_r)_{1 \leq r \leq t-1}$ and we want to make the estimation using $X_t$ for $Y_t$. This interval could guarantee the \$ P(Y_t \in \hat{C}*t)* \geq 1 - \alpha\$. That framework is used when the data are considered "exchangeable" that means let $(V_1,V_2,.. V_n)$ our data, we establish that for any permutation $\pi$, $(V_1,V_2,.. V_n)=^d (V_\pi1,V_\pi2,.. V_\pi n)$. The standard conformal prediction works in an easy way as explained below. So, given the value of $Y_t$, we want to predict it using the $X_t$. Let y a candidate for it, we need to construct a predictor $\hat\mu(X)$ (remember that in conformal prediction we care about the functional form of the predictor and it can assume whichever distribution we want) and we need to evaluate the scores $S(X_t,y)= \mid \hat\mu(X)- y \mid$ that are a measure of conformity of the predicted y with respect to the observed data. In that case we have performed the mean as point predictor, but for instance we could apply the quantile regression in order to achieve not only the marginal coverage, but the conditional coverage as well. Through the quantile regression we are going to get better conformal intervals since we are able to guarantee the coverage conditioning on a particular value of the covariate. Therefore, we care of the heteroscedasticity in the data. This method is based on the estimation of the quantile of the distribution $\hat q(X;p)$ where p is p-th quantile of the distribution $Y \mid X$. Suppose that we have performed the division of the training data from the calibration data and we assume that both the training and the calibration dataset are exchangeable. We now perform the the split conformal prediction. The estimation of the upper quantile is basically given by $\hat q(X_t;\frac{\alpha}{2})$ and instead for the lower quantile we have $\hat q(X_t;\frac{1-\alpha}{2})$. The quantiles are calculated on the training set. The scores are equal to $S(X_t,y)=\text max(\hat q(X_t;\frac{\alpha}{2}) - y; y- \hat q(X_t;\frac{1-\alpha}{2}))$. They are calculated on the calibration dataset in order to give the same weight and to treat symmetrically all the data. Now, using the calibration set we are able to compute the critical quantile: $$
\hat{Q}(p) := \inf 
\left\{
s : 
(\frac{1}{|\mathcal{D}_{\text{cal}}|} 
\sum_{(X_r, Y_r) \in \mathcal{D}_{\text{cal}}} 
\mathbf{1}\{S(X_r, Y_r) \leq s) \geq p
\right\}$$ Basically with this critical quantile we are calculating the infimum value for the score in the calibration dataset such that the summation of the previous scores are less or equal to the actual score with probability equal to the p-th quantile or in other words, better aligned with the classes, we are looking for the the critical quantile that is equal to \$ S{\lfloor (1-\alpha)(n_c + 1) \rfloor} \$ (the score at the integer position calculated in the p-th quantile on the calibration dataset). There is another notation: $$
P(Y_t \in \hat{C}_t) = P(S(X_t, Y_t) \leq \hat{Q}(1-\alpha)) = \frac{\left\lfloor |\mathcal{D}_{\text{cal}}| (1 - \alpha) \right\rfloor}{|\mathcal{D}_{\text{cal}}|+1}$$

## Adaptive conformal prediction

Unfortunately in real-world applications, the exchangeability cannot be ensured like in the financial applications since when the covariate shift in distribution happens, we cannot guarantee anymore the exchangeability. That's why it is introduced the "adaptive conformal inference", that is able to ensure the validity through an online forecasting: it means that we construct conformal inference adapted to the changes in the data when they occur and it should be re-estimated to align with the most recent observations. This approach is simple, because it estimates only a single parameter and it is general since we could use any machine learning tools in order to compute the point prediction. So, now we estimate the scores function $S_t()$ and the $\hat Q_t$. In addition, in the adaptive conformal prediction we need to estimate the miscoverage rate of prediction given by $$
M_t(\alpha) := P(S_t(X_t, Y_t) > \hat{Q_t}(1 - \alpha)$$ So, $M_t$ is a set of all the values that are greater than the critical quantile and they should belong to the alpha quantile. This $M_t(\alpha)$ cannot be equal or close to $\alpha$, but it may exist another value $\alpha_t \in (0,1)$ such that $M_t(\alpha)$ is approximated to $\alpha$. Basically, we work by looking at the miscoverage rate of prediction and we need to define an indicator function for the errors. $$
err_t := 
\begin{cases}
1, & \text{if } Y_t \notin \hat{C}_t(\alpha_t), \\
0, & \text{otherwise}
\end{cases}
$$ where $$
\hat{C}_t(\alpha_t) := \{y : S_t(X_t, y) \leq \hat{Q}_t(1 - \alpha_t)\}
$$

If the errors are equal to 1 basically the confidence interval was too short and it is not able to capture all the errors; instead if it is equal to 0, the confidence interval was too long and it was not able to exclude the errors equal to $\alpha$.

In the experiment presented in the paper we start with $\alpha_1=\alpha$.

We apply the standard conformal prediction in the first case, and then we go through the adaptive conformal prediction with the $\alpha_t$. Then $\alpha_t= \alpha_t + \gamma(\alpha - err_t)$. $\gamma$ is a step-size parameter and it is greater than 0. It is a measure between adaptability and stability. If $\gamma$ is very high, we need to adapt more the greater change in the distribution, but it increases the $\alpha_t$ as well. We focus only on this case for simplicity.

But there is an extension as well: $\alpha_{t+1}= \alpha_t + \gamma (\alpha - \sum_ {s=1}^{t} w_s err_s)$ where $w_s {1 \leq s \leq t}$ with a sequence of increasing weights starting from $\sum_ {s=1}^{t} w_s = 1$.

#  2) Application in R

## Point 2.2 , 2.3 , 2.4

library(quantmod) library(quantreg) library(rugarch)

Since we are using a time series and the data are not exchangeable, the paper suggests to use the "adaptive" conformal prediction. We are going to make the first prediction by using the classical $\alpha$ and the we adapt this value step by step when new data arrives. The first adaptive method is the simplest by using $\alpha_t= \alpha +(\gamma -\text {err})$. The second method is the momentum. We do it for a non normalized and a normalized conformal prediction. A non-normalized conformal prediction follows the calculation done in the point 1 for both the scores and the quantile and it reaches the coverage, but that coverage is a marginal coverage. Instead, the normalized one performs the calculation of the scores by normalizing it with the variance and then also the quantile are calculated by multiplying them to the variance. **FORMULA**. In addition we have proposed a standard Garch model compared to a Garch that takes into account the leverage effect. We recall that leverage effect means that the negative events impact more than the positive ones, ending up in an asymmetric distribution with heavy left tails.

**WRITE THE FORMULA OF THE ADAPTATION BUT LOOK AT THE RETURNS**

Let's start.

We have defined a general function with different arguments:

1.  returns of our choice;

2.  $\alpha$ and $\gamma$;

3.  the horizon of our estimation;

4.  lookback are the number of steps back that we perform;

5.  model_spec and score_type are the ones that split the normalized to the non-normalized and the standard Garch and the asymmetric Garch;

6.  garchP is the arch order and garchQ is the garch order (in order to avoid misunderstings connected to the order);

7.  startUp is the starting point;

8.  updateMethod is used to exploit which method we are going to use to adapt our $\alpha$.

    We have not performed a split in the data (train and calibration data) because the paper suggested to use the entire dataset, but we recall that there is the possibility to split the data in order to use the split conformal prediction, even for the returns (since it is a very large dataset, so it could be well applied in that case!)

    For each step we have included a comment inside the box.

```{r}
garchConformalForcasting <- function(returns,alpha,gamma,lookback=1250, model_spec, score_type ,garchP=1,garchQ=1,startUp = 100,verbose=FALSE,updateMethod,momentumBW = 0.95){
  T <- length(returns)
  startUp <- max(startUp,lookback)
  
#We are specifying the model: we have used sgarch and aparch for the leverage effect
  if (model_spec == "sGARCH") {
    garchSpec <- ugarchspec(
      mean.model = list(armaOrder = c(0, 0), include.mean = FALSE),
      variance.model = list(model = "sGARCH", garchOrder = c(garchP, garchQ)),
      distribution.model = "norm"
    )
  } else if (model_spec == "apARCH") {
    garchSpec <- ugarchspec(
      mean.model = list(armaOrder = c(0, 0), include.mean = FALSE),
      variance.model = list(model = "apARCH", garchOrder = c(garchP, garchQ)),
      distribution.model = "norm"
    )
  }
#the first alpha is the fixed one and then it adapts
alphat <- alpha

#Initialize data storage variables
  errSeqOC <- rep(0,T-startUp+1)
  alphaSequence <- rep(alpha,T-startUp+1)
  scores <- rep(0,T-startUp+1)
  low_PI <- rep(NA, T-startUp+1)
  high_PI <- rep(NA, T-startUp+1)
  
#starting the loop
  for(t in startUp:T){
    if(verbose){
      print(t)
    }
#Then we have fitted the GARCH model and compute new conformity scores for horizon
    garchFit <- ugarchfit(garchSpec, returns[(t-lookback+1):(t-1) ],solver="hybrid")
    sigmaNext <- sigma(ugarchforecast(garchFit,n.ahead=1))

#then we start the loop in order to calculate the scores, that are the absolute deviation of the returns squared and the predictor (our forecasted variance) 
    if (score_type == "non normalized") {
      scores[t-startUp + 1] <- abs(returns[t]^2- sigmaNext^2)
      } else if (score_type == "normalized") {
        scores[t-startUp + 1] <- abs(returns[t]^2- sigmaNext^2)/sigmaNext^2}
    
    recentScores <- scores[max(t-startUp+1 - lookback + 1,1):(t-startUp)]
    
### Compute error for outlier conformity
     errSeqOC[t-startUp+1] <- as.numeric(scores[t-startUp + 1] > quantile(recentScores,1-alphat))
    
# Update alpha_t with the simple methods and momentum
   alphaSequence[t-startUp+1] <- alphat
    if(updateMethod=="Simple"){
      alphat <- alphat + gamma*(alpha - errSeqOC[t-startUp+1])
    }else if(updateMethod=="Momentum"){
      w <- rev(momentumBW^(1:(t-startUp+1)))
      w <- w/sum(w)
      alphat <- alphat + gamma*(alpha - sum(errSeqOC[1:(t-startUp+1)]*w))
    }
    if(t %% 100 == 0){
      print(sprintf("Done %g steps",t))
    }
  }
  
# Calculate prediction intervals
  if (score_type == "non normalized") {
    low_PI <- sigmaNext[t-startUp+1]^2 - quantile(recentScores, 1 - alphat)
    high_PI <- sigmaNext[t-startUp+1]^2 + quantile(recentScores, 1 - alphat)
  } else if (score_type == "normalized") {
    low_PI <- sigmaNext[t-startUp+1]^2 - quantile(recentScores, 1 - alphat) / sigmaNext[t-startUp+1]^2
    high_PI <- sigmaNext[t-startUp+1]^2 + quantile(recentScores, 1 - alphat) / sigmaNext[t-startUp+1]^2
  }
  
  return(list(alphaSequence = alphaSequence, errSeqOC = errSeqOC, low_PI = low_PI, high_PI = high_PI))
}
```
So, basically we have chosen to perform both the standard GARCH and the APARCH (the asymmetric ARCH).
We know that through the **standard GARCH** we are able to model the heteroscedasticity of the conditional variance, that in the returns is mainly caused by clustering of the volatility over the time: high level of volatility are connected to other high level volatility, determining the typically clusters in volatility.
 From the properties of the Garch model we know that it has $E(r_t)=0$, the variance equal to $\frac{w}{1-\alpha}$ and the $\text{Cov}=0$. The process is stationary, since it has constant mean, constant variance and costant covariance, for instance we could say that another stylized facts is that the return behaves like a random walk.
 What actually changes over the time is the conditional variance and it is defined in terms of variance as $\sigma_{t|t-1}= \sigma^2 + \alpha (r_{t-1} - \sigma^2)$.
 In order to model this particular shape we are going to use the GARCH model that defines the return as $r_t= \sigma_{t|t-1} \epsilon_t$ with $\epsilon_t = N(0,1)$. Its variance is 1 in order to avoid scales on the variance of the return.
 $\sigma_{t|t-1}= w + \alpha r_{t-1}^2$.
 But, it's true that the standard Garch is able to capture the clustering of the volatility, but it misses another stylized fact that is connected to the so-called leverage effect (this is basically connected to the asymmetric shape of the returns and the left tail is heavier than the right).
In the specific, the standard Garch shows the magnitude of the returns but we know that in the market the negative returns have a higher impact with the respect the positive ones.
So, the negative events can shock more the investor and this is not captured by standard Garch.
That's why we have applied the **APARCH model**, that is a model able to capture the negative returns without relying only on the magnitude of the returns.
 **FORMULA**
 In general to reach better conclusions we can perform same tests like *the sign bias test*, that has as null hypothesis the "no sign bias".
 It works by regressing multiple times the process with three different parameters: one for the sign bias, the other two for the magnitude of the asymmetry.
But since the main aim of this project is not finding which is the best Garch model we have chosen the standard one and the APARCH.
 Furthermore, we have defined a distinction between the normalized scores and the non-normalized ones: in the first case we have introduce $\sigma^2$, that is the parameter that forces the normalization of the scores $\frac{|S_i^2- \sigma^2|}{\sigma^2}$ and then also the interval is getting normalized by that parameter (in the specific the quantile is multiplied to $\sigma^2$).
 In the second case, we have used the classical approach.
 This slight difference among the two ways is connected on the fact that we have an issue in conformal prediction: we can guarantee the validity and the true coverage, but we cannot guarantee the conditional coverage.
 When we apply the non-normalized scores we can get "fixed" intervals (because the coverage is marginal); instead with the non-normalized we are more able to achieve an interval that is adapted to the specific behaviour of the time series.
 Let's apply the function above on the real data.
We have downloaded TSLA data and computed the returns.

```{r}
getSymbols("TSLA", from = "2016-01-01", to = "2024-11-30")
tsla = na.omit(Cl(TSLA))
returns = dailyReturn(tsla)

```

We have applied the function for each cases that we have considered above.

```{r}
aparch_nonnorm = garchConformalForecasting(returns = returns, alpha = 0.1, model_spec="apARCH", score_type="non normalized",lookback=1250, garchP=1, garchQ=1, gamma=0.001, startUp=100, updateMethod="Simple", n.ahead=50)
garch_norm =garchConformalForecasting(returns = returns, alpha = 0.1, model_spec="sGARCH", score_type="normalized",lookback=1250, garchP=1, garchQ=1, gamma=0.001, startUp=100, updateMethod="Simple", n.ahead=50)
garch_nonnorm = garchConformalForecasting(returns = returns, alpha = 0.1, model_spec="sGARCH", score_type="non normalized",lookback=1250, garchP=1, garchQ=1, gamma=0.001, startUp=100, updateMethod="Simple", n.ahead=50)
aparch_norm = garchConformalForecasting(returns = returns, alpha = 0.1, model_spec="apARCH", score_type="normalized",lookback=1250, garchP=1, garchQ=1, gamma=0.001, startUp=100, updateMethod="Simple",n.ahead=50)
```
 Now, let's go through the main plot for each application.
```{r} 
plotTimeSeriesWithBounds <- function(returns, low_PI, high_PI, startUp, title = "Time Series with Prediction Intervals") {
  # Number of forecasted points
  n <- ncol(low_PI)
  
  # Ensure the returns subset length matches the forecasted points
  returns_subset <- returns[(startUp + 1):(startUp + n)]
  returns_subset <- returns_subset[1:n]
  
  # Time indices for plotting
  time_indices <- seq(from = startUp + 1, length.out = n)
  
  # Calculate the central forecast (e.g., mean of bounds)
  central_forecast <- apply(rbind(low_PI, high_PI), 2, mean, na.rm = TRUE)
  
  # Plot
  plot(time_indices, as.numeric(returns_subset), type = "l", col = "blue", lwd = 2, 
       xlab = "Time", ylab = "Returns", main = title)
  matlines(time_indices, t(low_PI), col = "red", lty = 2, lwd = 1)  # Lower bounds
  matlines(time_indices, t(high_PI), col = "green", lty = 2, lwd = 1)  # Upper bounds
  lines(time_indices, central_forecast, col = "purple", lty = 1, lwd = 1.5)  # Central forecast
  
  legend("topright", legend = c("Returns", "Lower Bound", "Upper Bound", "Central Forecast"), 
         col = c("blue", "red", "green", "purple"), lty = c(1, 2, 2, 1), lwd = c(2, 1, 1, 1.5))
}

# plot 1 normalized garch
plotTimeSeriesWithBounds(
  returns = returns,
  low_PI = garch_norm$low_PI,
  high_PI = garch_norm$high_PI,
  startUp = 100,
  title = "Time Series with GARCH-based Prediction Intervals"
)

#plot 2 normalized aparch
plotTimeSeriesWithBounds(
  returns = returns,
  low_PI = aparch_norm$low_PI,
  high_PI = aparch_norm$high_PI,
  startUp = 100,
  title = "Normalized APARCH with ACI"
)
```
